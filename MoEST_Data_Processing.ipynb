{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1CIBAwSB1qniFyR_Q-szL1r1I2XmGbK8X",
      "authorship_tag": "ABX9TyPWqrDsFRPPLUYR4dySDZdW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kadefue/MoEST/blob/main/MoEST_Data_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import warnings\n",
        "\n",
        "# ==========================================\n",
        "# 0. Setup: Block Warnings\n",
        "# ==========================================\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# ==========================================\n",
        "# 1. Cleaning & Helper Functions\n",
        "# ==========================================\n",
        "\n",
        "def is_valid_text(value):\n",
        "    \"\"\"Checks if the value is Alphabetic or Alphanumeric.\"\"\"\n",
        "    s_val = str(value).strip()\n",
        "    if not s_val:\n",
        "        return False\n",
        "    clean_val = s_val.replace(\" \", \"\")\n",
        "    if clean_val.isalnum():\n",
        "        return True\n",
        "    if re.search(r'[a-zA-Z]', s_val):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def normalize_merged_cells(df, header_rows=15):\n",
        "    \"\"\"\n",
        "    Handles merged columns/rows in the header/label area.\n",
        "    Duplicates text horizontally and vertically for merged cells.\n",
        "    \"\"\"\n",
        "    if df.empty:\n",
        "        return df\n",
        "\n",
        "    limit = min(header_rows, len(df))\n",
        "    subset = df.iloc[:limit].copy()\n",
        "\n",
        "    # Forward fill horizontally and vertically\n",
        "    subset = subset.ffill(axis=1)\n",
        "    subset = subset.ffill(axis=0)\n",
        "\n",
        "    df.iloc[:limit] = subset\n",
        "    return df\n",
        "\n",
        "def process_council_sheet(df):\n",
        "    \"\"\"\n",
        "    Applies specific cleaning steps:\n",
        "    1. Populate structural columns downwards.\n",
        "    2. Remove 'Grand' and 'Total' rows.\n",
        "    (Sparsity and Row checks are now moved to the final stage)\n",
        "    \"\"\"\n",
        "    if df.empty:\n",
        "        return df\n",
        "\n",
        "    # Identify Region Column\n",
        "    region_col = None\n",
        "    for i, row in df.head(5).iterrows():\n",
        "        for col in df.columns:\n",
        "            val = str(row[col]).lower()\n",
        "            if \"region\" in val or \"mkoa\" in val:\n",
        "                region_col = col\n",
        "                break\n",
        "        if region_col is not None:\n",
        "            break\n",
        "\n",
        "    if region_col is None and not df.empty:\n",
        "        region_col = df.columns[0]\n",
        "\n",
        "    # Populate Columns Downwards (Unmerge Vertical for structural cols)\n",
        "    cols_to_fill = list(df.columns[:3])\n",
        "    if region_col is not None and region_col not in cols_to_fill:\n",
        "        cols_to_fill.append(region_col)\n",
        "\n",
        "    for col in cols_to_fill:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].replace({0: None, '0': None})\n",
        "            df[col] = df[col].ffill()\n",
        "\n",
        "    # \"Grand\" Logic\n",
        "    if region_col is not None and region_col in df.columns:\n",
        "        grand_mask = df[region_col].astype(str).str.contains(\"Grand\", case=False, na=False)\n",
        "        if grand_mask.any():\n",
        "            cutoff_idx = grand_mask.idxmax()\n",
        "            df = df.loc[:cutoff_idx-1]\n",
        "\n",
        "    # \"Total\" Logic (Region)\n",
        "    if region_col is not None and region_col in df.columns:\n",
        "        total_mask = df[region_col].astype(str).str.contains(\"Total\", case=False, na=False)\n",
        "        df = df[~total_mask]\n",
        "\n",
        "    # \"Total\" Logic (Council)\n",
        "    council_col = None\n",
        "    council_keywords = ['council', 'halmashauri', 'district', 'lga', 'wilaya', 'municipal', 'town council']\n",
        "\n",
        "    for i, row in df.head(5).iterrows():\n",
        "        for col in df.columns:\n",
        "            val = str(row[col]).lower()\n",
        "            if any(kw in val for kw in council_keywords):\n",
        "                council_col = col\n",
        "                break\n",
        "        if council_col is not None:\n",
        "            break\n",
        "\n",
        "    if council_col is not None and council_col in df.columns:\n",
        "        pat = \"Total|Sub-Total|Sub Total\"\n",
        "        council_total_mask = df[council_col].astype(str).str.contains(pat, case=False, na=False)\n",
        "        df = df[~council_total_mask]\n",
        "\n",
        "    # Final Cleanup: Remove rows with \"Total\" in first few columns\n",
        "    target_indices = [0, 1, 2]\n",
        "    for idx in target_indices:\n",
        "        if idx < len(df.columns):\n",
        "            col_name = df.columns[idx]\n",
        "            mask = df[col_name].astype(str).str.contains(\"Total\", case=False, na=False)\n",
        "            df = df[~mask]\n",
        "\n",
        "    # Duplicate Region Column Check\n",
        "    if region_col is not None and region_col in df.columns:\n",
        "        cols_to_drop = []\n",
        "        for col in df.columns:\n",
        "            if col == region_col: continue\n",
        "            if df[col].equals(df[region_col]):\n",
        "                cols_to_drop.append(col)\n",
        "        if cols_to_drop:\n",
        "            df = df.drop(columns=cols_to_drop)\n",
        "\n",
        "    return df\n",
        "\n",
        "def row_has_numeric(series):\n",
        "    \"\"\"Returns True if the row contains any numeric value.\"\"\"\n",
        "    for val in series:\n",
        "        if isinstance(val, (int, float)) and not isinstance(val, bool):\n",
        "            return True\n",
        "        if isinstance(val, str):\n",
        "            s = val.strip().replace(',', '')\n",
        "            if s.replace('.', '', 1).isdigit():\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "def perform_final_cleanup(df):\n",
        "    \"\"\"\n",
        "    Executes the final requested operations:\n",
        "    1. Delete rows with only 1 cell of value (excluding the Source_Year column).\n",
        "    2. Delete columns with >15% empty cells.\n",
        "    \"\"\"\n",
        "    if df.empty:\n",
        "        return df\n",
        "\n",
        "    # --- 1. ROW CLEANUP ---\n",
        "    # We check non-null count on all columns EXCEPT 'Source_Year'\n",
        "    # If a row has <= 1 valid data cell, we drop it.\n",
        "    cols_to_check = [c for c in df.columns if c != 'Source_Year']\n",
        "\n",
        "    # Calculate non-nulls row-wise for data columns\n",
        "    # We treat empty strings '' as Null here just in case\n",
        "    temp_df = df[cols_to_check].replace('', None)\n",
        "    row_counts = temp_df.notna().sum(axis=1)\n",
        "\n",
        "    # Keep rows where we have MORE than 1 data value\n",
        "    initial_rows = len(df)\n",
        "    df = df[row_counts > 1]\n",
        "    dropped_rows = initial_rows - len(df)\n",
        "    if dropped_rows > 0:\n",
        "        print(f\"    (Cleaned {dropped_rows} rows having <= 1 data value)\")\n",
        "\n",
        "    # --- 2. COLUMN SPARSITY (15% Threshold) ---\n",
        "    # Remove columns which have MORE than 15% empty cells\n",
        "    threshold = 0.90\n",
        "    initial_cols = len(df.columns)\n",
        "\n",
        "    # Calculate null percentage\n",
        "    # We treat 0 and '0' as valid values here? Usually yes, 0 is data.\n",
        "    # But often empty strings are loaded as objects. Let's stick to standard NaNs/None.\n",
        "    # If you consider '0' as empty, uncomment the replacement line below.\n",
        "    # df_check = df.replace({0: None, '0': None, '': None})\n",
        "\n",
        "    missing_pct = df.isna().mean()\n",
        "    cols_to_keep = missing_pct[missing_pct <= threshold].index\n",
        "\n",
        "    df = df[cols_to_keep]\n",
        "    dropped_cols = initial_cols - len(df.columns)\n",
        "    if dropped_cols > 0:\n",
        "        print(f\"    (Dropped {dropped_cols} columns with >15% empty cells)\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# ==========================================\n",
        "# 2. Main Extraction and Combination Logic\n",
        "# ==========================================\n",
        "\n",
        "def extract_and_combine_all():\n",
        "    base_dir = \"/content/drive/MyDrive/BEST\"\n",
        "\n",
        "    # --- DEFINITION: Data Categories and Mappings ---\n",
        "\n",
        "    # 1. Laboratories\n",
        "    lab_mapping = {\n",
        "        2016: [\"3.26Lab\", \"3.27LabGov\"],\n",
        "        2017: [\"3.36LabRegCoun\", \"3.37LabGovtRegCoun\"],\n",
        "        2018: [\"3.37Lab\", \"3.38LabGov\"],\n",
        "        2019: [\"3.37Lab\", \"3.38LabGov\"],\n",
        "        2020: [\"3.37Lab\", \"3.38LabGov\"],\n",
        "        2021: [\"T3.37Lab\", \"T3.38LabGov\"],\n",
        "        2022: [\"T3.38Lab\", \"T3.39LabGov\"],\n",
        "        2023: [\"T3.39LabG&NG\", \"T3.40LabG\"],\n",
        "        2024: [\"T3.39LabG&NG\", \"T3.40LabG\"],\n",
        "        2025: [\"T3.40LabG&NG\", \"T3.41LabG\"]\n",
        "    }\n",
        "\n",
        "    # 2. ICT Equipment\n",
        "    ict_mapping = {\n",
        "        2017: [\"T3.42ICTAllRegCoun\", \"T3.43ICTGovRegCoun\"],\n",
        "        2018: [\"T2.42_ICT_G&N\", \"T2.43_ICT_G\"],\n",
        "        2019: [\"Table170\", \"Table169\"],\n",
        "        2020: [\"Table147\", \"Table148\"],\n",
        "        2021: [\"Table155\", \"Table156\"],\n",
        "        2022: [\"T2.43_ICT_G&N\", \"T2.44_ICT_G\"],\n",
        "        2023: [\"T2.44_ICT_G&N\", \"T2.45_ICT_G\"],\n",
        "        2024: [\"T2.44_ICT_G&N\", \"T2.45_ICT_G\"],\n",
        "        2025: [\"T3.46ICT\", \"T3.47ICT_Gov\"]\n",
        "    }\n",
        "\n",
        "    # 3. Electricity\n",
        "    elec_mapping = {\n",
        "        2017: [\"T3.40SchElecAllRegCoun\", \"T3.41SchElecGovRegCoun\"],\n",
        "        2018: [\"T2.44_Elect_G&N\", \"T2.45_Elect_G\"],\n",
        "        2019: [\"Table165\", \"Table167\"],\n",
        "        2020: [\"Table145\", \"Table146\"],\n",
        "        2021: [\"Table152\", \"Table153\"],\n",
        "        2022: [\"T2.41_Elect_G&N\", \"T2.42_Elect_G\"],\n",
        "        2023: [\"T2.42_Elect_G&N\", \"T2.43_Elect_G\"],\n",
        "        2024: [\"T2.42_Elect_G&N\", \"T2.43_Elect_G\"],\n",
        "        2025: [\"T2.42_Elect_G&N\", \"T2.43_Elect_G\"]\n",
        "    }\n",
        "\n",
        "    tasks = [\n",
        "        (\"Laboratories\", lab_mapping, \"Combined_Laboratories_All_G_NG.csv\", \"Combined_Laboratories_Govt.csv\"),\n",
        "        (\"ICT_Equipment\", ict_mapping, \"Combined_ICT_All_G_NG.csv\", \"Combined_ICT_Govt.csv\"),\n",
        "        (\"Electricity\", elec_mapping, \"Combined_Electricity_All_G_NG.csv\", \"Combined_Electricity_Govt.csv\")\n",
        "    ]\n",
        "\n",
        "    for category_name, mapping, left_out, right_out in tasks:\n",
        "        print(f\"\\n=======================================================\")\n",
        "        print(f\" PROCESSING CATEGORY: {category_name}\")\n",
        "        print(f\"=======================================================\")\n",
        "\n",
        "        left_dfs = []\n",
        "        right_dfs = []\n",
        "\n",
        "        processed_first_file = False\n",
        "\n",
        "        for year, sheets in sorted(mapping.items()):\n",
        "            filename = f\"BEST {year}.xlsx\"\n",
        "            file_path = os.path.join(base_dir, filename)\n",
        "\n",
        "            if not os.path.exists(file_path):\n",
        "                print(f\"Year {year}: File not found ({filename})\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                xls = pd.ExcelFile(file_path)\n",
        "                available_sheets = xls.sheet_names\n",
        "\n",
        "                # Normalize sheet names for robust lookup (ignore spaces)\n",
        "                normalized_lookup = {s.replace(\" \", \"\"): s for s in available_sheets}\n",
        "\n",
        "                # Helper to process a single sheet\n",
        "                def process_sheet_data(target_name_clean, is_first_time):\n",
        "                    if target_name_clean in normalized_lookup:\n",
        "                        real_sheet_name = normalized_lookup[target_name_clean]\n",
        "\n",
        "                        # Load & Clean\n",
        "                        df = pd.read_excel(xls, sheet_name=real_sheet_name, header=None)\n",
        "                        df = normalize_merged_cells(df)\n",
        "                        df = process_council_sheet(df)\n",
        "\n",
        "                        # --- SMART ROW DELETION (Before adding Year) ---\n",
        "                        status_suffix = \"\"\n",
        "                        if not is_first_time:\n",
        "                            check_limit = min(4, len(df))\n",
        "                            top_slice = df.iloc[:check_limit]\n",
        "                            rest_slice = df.iloc[check_limit:]\n",
        "\n",
        "                            rows_to_keep = []\n",
        "                            for idx in range(len(top_slice)):\n",
        "                                row_data = top_slice.iloc[idx]\n",
        "                                if row_has_numeric(row_data):\n",
        "                                    rows_to_keep.append(top_slice.iloc[[idx]])\n",
        "\n",
        "                            if rows_to_keep:\n",
        "                                df = pd.concat(rows_to_keep + [rest_slice])\n",
        "                                dropped_count = check_limit - len(rows_to_keep)\n",
        "                                status_suffix = f\"(Dropped {dropped_count} header rows, kept {len(rows_to_keep)} numeric rows)\"\n",
        "                            else:\n",
        "                                df = rest_slice\n",
        "                                status_suffix = f\"(Dropped top {check_limit} header rows)\"\n",
        "                        else:\n",
        "                            status_suffix = \"(First file: All rows kept)\"\n",
        "\n",
        "                        # --- ADD YEAR COLUMN ---\n",
        "                        df.insert(0, 'Source_Year', year)\n",
        "\n",
        "                        print(f\"  {year}: [FOUND] '{real_sheet_name}' {status_suffix}\")\n",
        "                        return df\n",
        "                    else:\n",
        "                        print(f\"  {year}: [MISSING] '{target_name_clean}'\")\n",
        "                        return None\n",
        "\n",
        "                # --- EXTRACT LEFT ---\n",
        "                df_left = process_sheet_data(sheets[0], not processed_first_file)\n",
        "                if df_left is not None:\n",
        "                    left_dfs.append(df_left)\n",
        "\n",
        "                # --- EXTRACT RIGHT ---\n",
        "                if len(sheets) > 1:\n",
        "                    df_right = process_sheet_data(sheets[1], not processed_first_file)\n",
        "                    if df_right is not None:\n",
        "                        right_dfs.append(df_right)\n",
        "\n",
        "                # Mark success\n",
        "                if df_left is not None or (len(sheets) > 1 and df_right is not None):\n",
        "                    processed_first_file = True\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  {year}: [ERROR] Processing file: {e}\")\n",
        "\n",
        "        # --- SAVE FILES FOR THIS CATEGORY ---\n",
        "        print(f\"\\n--- Finalizing & Saving {category_name} ---\")\n",
        "\n",
        "        if left_dfs:\n",
        "            combined_left = pd.concat(left_dfs, ignore_index=True)\n",
        "            # PERFORM FINAL CLEANUP\n",
        "            combined_left = perform_final_cleanup(combined_left)\n",
        "            combined_left.to_csv(left_out, index=False, header=False)\n",
        "            print(f\"  -> Saved '{left_out}' ({len(combined_left)} rows)\")\n",
        "        else:\n",
        "            print(f\"  -> No data for '{left_out}'\")\n",
        "\n",
        "        if right_dfs:\n",
        "            combined_right = pd.concat(right_dfs, ignore_index=True)\n",
        "            # PERFORM FINAL CLEANUP\n",
        "            combined_right = perform_final_cleanup(combined_right)\n",
        "            combined_right.to_csv(right_out, index=False, header=False)\n",
        "            print(f\"  -> Saved '{right_out}' ({len(combined_right)} rows)\")\n",
        "\n",
        "    print(\"\\n=== All Tasks Complete ===\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    extract_and_combine_all()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b539eff-9935-4654-b057-329832b3f02d",
        "id": "a3klvnzcD-MV"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=======================================================\n",
            " PROCESSING CATEGORY: Laboratories\n",
            "=======================================================\n",
            "  2016: [FOUND] '3.26Lab' (First file: All rows kept)\n",
            "  2016: [FOUND] '3.27LabGov' (First file: All rows kept)\n",
            "  2017: [FOUND] '3.36LabRegCoun' (Dropped top 4 header rows)\n",
            "  2017: [FOUND] '3.37LabGovtRegCoun' (Dropped top 4 header rows)\n",
            "  2018: [FOUND] '3.37Lab' (Dropped top 4 header rows)\n",
            "  2018: [FOUND] '3.38LabGov' (Dropped top 4 header rows)\n",
            "  2019: [FOUND] '3.37Lab' (Dropped 2 header rows, kept 2 numeric rows)\n",
            "  2019: [FOUND] '3.38LabGov' (Dropped 2 header rows, kept 2 numeric rows)\n",
            "  2020: [FOUND] '3.37Lab' (Dropped 2 header rows, kept 2 numeric rows)\n",
            "  2020: [FOUND] '3.38LabGov' (Dropped 2 header rows, kept 2 numeric rows)\n",
            "  2021: [FOUND] 'T3.37Lab' (Dropped 2 header rows, kept 2 numeric rows)\n",
            "  2021: [FOUND] 'T3.38LabGov' (Dropped 2 header rows, kept 2 numeric rows)\n",
            "  2022: [FOUND] 'T3.38Lab' (Dropped top 4 header rows)\n",
            "  2022: [FOUND] 'T3.39LabGov' (Dropped top 4 header rows)\n",
            "  2023: [FOUND] 'T3.39LabG&NG' (Dropped top 4 header rows)\n",
            "  2023: [FOUND] 'T3.40LabG' (Dropped top 4 header rows)\n",
            "  2024: [FOUND] 'T3.39LabG&NG' (Dropped top 4 header rows)\n",
            "  2024: [FOUND] 'T3.40LabG' (Dropped top 4 header rows)\n",
            "  2025: [FOUND] 'T3.40LabG&NG' (Dropped top 4 header rows)\n",
            "  2025: [FOUND] 'T3.41LabG' (Dropped top 4 header rows)\n",
            "\n",
            "--- Finalizing & Saving Laboratories ---\n",
            "    (Cleaned 6 rows having <= 1 data value)\n",
            "    (Dropped 1 columns with >15% empty cells)\n",
            "  -> Saved 'Combined_Laboratories_All_G_NG.csv' (1840 rows)\n",
            "    (Cleaned 6 rows having <= 1 data value)\n",
            "    (Dropped 2 columns with >15% empty cells)\n",
            "  -> Saved 'Combined_Laboratories_Govt.csv' (1840 rows)\n",
            "\n",
            "=======================================================\n",
            " PROCESSING CATEGORY: ICT_Equipment\n",
            "=======================================================\n",
            "  2017: [FOUND] 'T3.42ICTAllRegCoun' (First file: All rows kept)\n",
            "  2017: [FOUND] 'T3.43ICTGovRegCoun' (First file: All rows kept)\n",
            "  2018: [FOUND] 'T2.42_ICT_G&N' (Dropped 3 header rows, kept 1 numeric rows)\n",
            "  2018: [FOUND] 'T2.43_ICT_G' (Dropped 3 header rows, kept 1 numeric rows)\n",
            "  2019: [FOUND] 'Table170' (Dropped 1 header rows, kept 3 numeric rows)\n",
            "  2019: [FOUND] 'Table169' (Dropped 1 header rows, kept 3 numeric rows)\n",
            "  2020: [FOUND] 'Table147' (Dropped 1 header rows, kept 3 numeric rows)\n",
            "  2020: [FOUND] 'Table148' (Dropped 1 header rows, kept 3 numeric rows)\n",
            "  2021: [FOUND] 'Table155' (Dropped 1 header rows, kept 3 numeric rows)\n",
            "  2021: [FOUND] 'Table156' (Dropped 1 header rows, kept 3 numeric rows)\n",
            "  2022: [FOUND] 'T2.43_ICT_G&N' (Dropped 3 header rows, kept 1 numeric rows)\n",
            "  2022: [FOUND] 'T2.44_ICT_G' (Dropped 3 header rows, kept 1 numeric rows)\n",
            "  2023: [FOUND] 'T2.44_ICT_G&N' (Dropped 3 header rows, kept 1 numeric rows)\n",
            "  2023: [FOUND] 'T2.45_ICT_G' (Dropped 3 header rows, kept 1 numeric rows)\n",
            "  2024: [FOUND] 'T2.44_ICT_G&N' (Dropped 3 header rows, kept 1 numeric rows)\n",
            "  2024: [FOUND] 'T2.45_ICT_G' (Dropped 3 header rows, kept 1 numeric rows)\n",
            "  2025: [FOUND] 'T3.46ICT' (Dropped 3 header rows, kept 1 numeric rows)\n",
            "  2025: [FOUND] 'T3.47ICT_Gov' (Dropped 3 header rows, kept 1 numeric rows)\n",
            "\n",
            "--- Finalizing & Saving ICT_Equipment ---\n",
            "    (Cleaned 14 rows having <= 1 data value)\n",
            "  -> Saved 'Combined_ICT_All_G_NG.csv' (1658 rows)\n",
            "    (Cleaned 14 rows having <= 1 data value)\n",
            "  -> Saved 'Combined_ICT_Govt.csv' (1658 rows)\n",
            "\n",
            "=======================================================\n",
            " PROCESSING CATEGORY: Electricity\n",
            "=======================================================\n",
            "  2017: [FOUND] 'T3.40SchElecAllRegCoun' (First file: All rows kept)\n",
            "  2017: [FOUND] 'T3.41SchElecGovRegCoun' (First file: All rows kept)\n",
            "  2018: [FOUND] 'T2.44_Elect_G&N' (Dropped top 4 header rows)\n",
            "  2018: [FOUND] 'T2.45_Elect_G' (Dropped top 4 header rows)\n",
            "  2019: [FOUND] 'Table165' (Dropped 0 header rows, kept 4 numeric rows)\n",
            "  2019: [FOUND] 'Table167' (Dropped 2 header rows, kept 2 numeric rows)\n",
            "  2020: [FOUND] 'Table145' (Dropped 0 header rows, kept 4 numeric rows)\n",
            "  2020: [FOUND] 'Table146' (Dropped 2 header rows, kept 2 numeric rows)\n",
            "  2021: [FOUND] 'Table152' (Dropped 0 header rows, kept 4 numeric rows)\n",
            "  2021: [FOUND] 'Table153' (Dropped 2 header rows, kept 2 numeric rows)\n",
            "  2022: [FOUND] 'T2.41_Elect_G&N' (Dropped 2 header rows, kept 2 numeric rows)\n",
            "  2022: [FOUND] 'T2.42_Elect_G' (Dropped 2 header rows, kept 2 numeric rows)\n",
            "  2023: [FOUND] 'T2.42_Elect_G&N' (Dropped 2 header rows, kept 2 numeric rows)\n",
            "  2023: [FOUND] 'T2.43_Elect_G' (Dropped 2 header rows, kept 2 numeric rows)\n",
            "  2024: [FOUND] 'T2.42_Elect_G&N' (Dropped 2 header rows, kept 2 numeric rows)\n",
            "  2024: [FOUND] 'T2.43_Elect_G' (Dropped 2 header rows, kept 2 numeric rows)\n",
            "  2025: [FOUND] 'T2.42_Elect_G&N' (Dropped 2 header rows, kept 2 numeric rows)\n",
            "  2025: [FOUND] 'T2.43_Elect_G' (Dropped 2 header rows, kept 2 numeric rows)\n",
            "\n",
            "--- Finalizing & Saving Electricity ---\n",
            "    (Cleaned 7 rows having <= 1 data value)\n",
            "  -> Saved 'Combined_Electricity_All_G_NG.csv' (1658 rows)\n",
            "    (Cleaned 6 rows having <= 1 data value)\n",
            "  -> Saved 'Combined_Electricity_Govt.csv' (1660 rows)\n",
            "\n",
            "=== All Tasks Complete ===\n"
          ]
        }
      ]
    }
  ]
}